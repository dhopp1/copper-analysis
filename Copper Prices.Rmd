---
title: "Time Series Analysis of Copper Prices"
author: "Daniel Hopp"
date: "4 January 2019"
output:
  html_document:
    code_folding: hide
---

## Exposition of the Problem

The goal of this analysis is to get a better sense of how copper prices behave, with the ultimate goal of being able to identify, with a reasonable degree of certainty, if a day's price is the lowest that can be expected over a 4 week period.  Of course the price of copper is dependent on many different factors, including prices/indicators of other commodities, inventory levels, regulatory announcements, geopolitical conditions in source countries, and indicators of downstream industries.  However for simplicity and ease of use/interpretation, this analysis will stick with only copper's past prices as indicators of its future direction.

### Data

Historical data on copper prices was obtained from: https://www.macrotrends.net/1476/copper-prices-historical-chart-data, which has daily copper prices (USD/pound) starting from 2 July 1959. The source makes no mention of inflation, but we will assume prices are in constant dollars. A simple plot of the raw data will give us an initial idea of how copper prices have developed over time. Recessions are also shaded to give an indication of copper prices' relation to overall economic performance.

```{r eval = T}
library(ggplot2)
library(tseries)
library(lmtest)
library(astsa)

data <- read.csv('hist copper.csv')

#converting date to datetime data type
data$date <- as.Date(data$date)

#data on recessions for plot
recessions.df = read.table(textConnection(
"Peak, Trough
1960-04-01, 1961-02-01
1969-12-01, 1970-11-01
1973-11-01, 1975-03-01
1980-01-01, 1980-07-01
1981-07-01, 1982-11-01
1990-07-01, 1991-03-01
2001-03-01, 2001-11-01
2007-12-01, 2009-06-01"), sep=',',
colClasses=c('Date', 'Date'), header=TRUE)
recessions.trim = subset(recessions.df, Peak >= min(data$date))

#function for simple line plot
line_plot <- function(data, date, value, title, xlab, ylab){
  plot <- ggplot(data = data, aes(x = date, y = value, group = 1)) + 
  geom_line() +
  ggtitle(title) +
  xlab(xlab) + 
  ylab(ylab)
  plot
}

line_plot(data, date, value, "Daily Copper Prices (USD/Pound)", "Date", "USD/Pound") +
  geom_rect(data=recessions.trim, aes(xmin=Peak, xmax=Trough, ymin=-Inf, ymax=+Inf), alpha=0.2, inherit.aes = FALSE)
```

#### Interpretation
We see that on a macro level, copper prices don't seem to follow any readily discernible pattern, beyond the fact that they go down (maybe with some delay) during recessions, so some quantitative time series analyses will need to be done.  But first, to get a better look at how the prices behave on a smaller time scale, we will look at the plot of the last 6 months.

```{r eval = T}
line_plot(data[data$date >= "2018-07-01",], date, value, "Daily Copper Prices (USD/Pound)", "Date", "USD/Pound")
```

#### Interpretation
On a micro level, the series seems to have a certain degree of mean correction, making a random walk unlikely. However to get a better idea of the time series' nature, more analyses will need to be performed.

### Copper Prices by Day of Week
Before estimating a time series model, we will look at the development of copper prices by day of week. The hypothesis is that prices may display a weekly seasonality and thus there is an ideal day of the week to buy. To examine this we'll first look very generally at mean prices by day of week.

```{r eval = T}
#get day of week
data$dow <- weekdays(data$date)
monday <- data[2,"dow"]
tuesday <- data[3,"dow"]
wednesday <- data[4,"dow"]
thursday <- data[5,"dow"]
friday <- data[6,"dow"]

library(scales)
#bar graph for all time
all <- data[data$date >= '1949-01-01',]
bar_data <- aggregate(all$value, list(all$dow), mean)
bar_data$Group.1 <- factor(bar_data$Group.1, levels = c(monday, tuesday, wednesday, thursday, friday))
ggplot(data = bar_data, aes(x = Group.1, y = x)) + 
  geom_col(fill = "#FF6666") + 
  geom_text(aes(label = paste("$", round(bar_data$x, 3), sep="")), vjust = -0.5) + 
  scale_y_continuous(limits=c(1,1.4),oob = rescale_none) + 
  ggtitle("Average Price per Day of Week, 1959-2018") + 
  xlab("Day of Week") + 
  ylab("Average Price")

#bar graph for 2010-2018
all <- data[data$date >= '2010-01-01',]
bar_data <- aggregate(all$value, list(all$dow), mean)
bar_data$Group.1 <- factor(bar_data$Group.1, levels = c(monday, tuesday, wednesday, thursday, friday))
ggplot(data = bar_data, aes(x = Group.1, y = x)) + 
  geom_col(fill = "#5d9afc") + 
  geom_text(aes(label = paste("$", round(bar_data$x, 3), sep="")), vjust = -0.5) + 
  scale_y_continuous(limits=c(2.8,3.2),oob = rescale_none) + 
  ggtitle("Average Price per Day of Week, 2010-2018") + 
  xlab("Day of Week") + 
  ylab("Average Price")
```
#### Interpretation
Over the whole data set we see a slight trend of increasing prices during the course of the week, with cheapest prices on Monday (by about only 1 or 2 cents) and most expensive on Monday. To make sure this wasn't a time dependent anomaly, the same graph was produced for only 2010-2018. We see the same trend but weaker. Despite the existance of this trend, the evidence is not strong enough to be able to make a blanket recommendation for simply buying earlier in the week. So we move on to the time series analysis.

### Time Series Model
We will now transform the data to obtain a stationary series. We already saw from our initial plot that the raw data is not a stationary time series, as there are clear trends. Our initial impression can be verified using an Augmented Dickey-Fuller Test for a unit root, obtaining a p-value of `r round(adf.test(data$value)$p.value, 2)`. This is not a sufficient to reject the null hypothesis of a non-stationary series, so first we will try first differencing to see if we then obtain a stationary process. The result of that transformation has a Dickey-Fuller p-value of `r round(adf.test(diff(data$value))$p.value, 2)`, indicating stationarity and is plotted below.

```{r eval = T}
ggplot(data = data, aes(x = date, y = c(0, diff(data$value)))) + 
  geom_line() + 
  ggtitle("First Difference of Copper Prices") + 
  xlab("Date") + 
  ylab("First Difference of Price")
```

#### Interpretation
Any trends are gone, however we see a high degree of heteroskedasticity even without any tests, with variance increasing with time. This tells us we need to log transform the series, then first difference, which is plotted below.

```{r eval = T}
ggplot(data = data, aes(x = date, y = c(0, diff(log(data$value))))) + 
  geom_line() + 
  ggtitle("First Difference of Logged Copper Prices") + 
  xlab("Date") + 
  ylab("First Difference of Log Price")

gqtest(diff(log(data$value))~1)$p.value
```

#### Interpretation
To the eye it appears that the series is no longer heteroskedastic, but that can be verified with a test. The Goldfeld-Quandt test for heteroskedasticity obtains a p-value of `r round(gqtest(diff(log(data$value))~1)$p.value, 2)`, which is >.05 and allows us to accept the null hypothesis that variance does not change between segments, and thus our series is homoskedastic. Now we can use R's SARIMA function to estimate a time series model for the data.

```{r eval = T}
library(forecast)

#create time series data
ts <- ts(data = diff(log(data$value)), start = c(1959, 125), end = c(2019, 1), frequency = 250)

#model <- auto.arima(ts, start.p = 1)
model <- sarima.for(ts, 1, 0, 0, n.ahead = 10)
```